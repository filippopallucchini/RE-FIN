{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(1)): # num epochs\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs = {\"input_ids\": batch[0].to(device),\n",
    "                      \"attention_mask\": batch[1].to(device),\n",
    "                      \"labels\": batch[2].to(device)}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs) # forward pass\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step() # update weights\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# Define the testing function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = {\"input_ids\": batch[0].to(device),\n",
    "                      \"attention_mask\": batch[1].to(device),\n",
    "                      \"labels\": batch[2].to(device)}\n",
    "            outputs = model(**inputs)\n",
    "            _, predicted = torch.max(outputs.logits, dim=1)\n",
    "            total += inputs[\"labels\"].size(0)\n",
    "            correct += (predicted == inputs[\"labels\"]).sum().item()\n",
    "    print(\"Accuracy: {:.3f}\".format(correct / total))\n",
    "\n",
    "#CUDA_LAUNCH_BLOCKING=1\n",
    "# Set seed for reproducibility\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "#define path\n",
    "path = 'your_path'\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(f\"{path}/OUTPUT/fiqa_enriched_allagree.csv\", usecols=[1,2],\n",
    "                 header=None, skiprows=1, names=['sentence', 'label'], nrows=771)\n",
    "\n",
    "df.loc[df['label'] == 2, 'label'] = 1\n",
    "\n",
    "#dataset_origin = load_dataset(\"ChanceFocus/fiqa-sentiment-classification\")\n",
    "#dataset_sentences = []\n",
    "#dataset_labels = []\n",
    "#for i in dataset_origin['train']:\n",
    "#    if i['score']>0.3:\n",
    "#        dataset_sentences.append(i['sentence'])\n",
    "#        dataset_labels.append(1)\n",
    "#    elif i['score']<-0.3:\n",
    "#        dataset_sentences.append(i['sentence'])\n",
    "#        dataset_labels.append(0)\n",
    "#        \n",
    "#for i in dataset_origin['valid']:\n",
    "#    if i['score']>0.3:\n",
    "#        dataset_sentences.append(i['sentence'])\n",
    "#        dataset_labels.append(1)\n",
    "#    elif i['score']<-0.3:\n",
    "#        dataset_sentences.append(i['sentence'])\n",
    "#        dataset_labels.append(0)\n",
    "#        \n",
    "#for i in dataset_origin['test']:\n",
    "#    if i['score']>0.3:\n",
    "#        dataset_sentences.append(i['sentence'])\n",
    "#        dataset_labels.append(1)\n",
    "#    elif i['score']<-0.3:\n",
    "#        dataset_sentences.append(i['sentence'])\n",
    "#        dataset_labels.append(0)\n",
    "#\n",
    "#df = pd.DataFrame({'text': dataset_sentences, 'label': dataset_labels})\n",
    "\n",
    "#da qui uguale per tutti\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "# Split into train and test sets\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(df['sentence'].to_numpy(), df['label'].to_numpy(), test_size=0.2, random_state=seed_value)\n",
    "\n",
    "# Load the tokenizer and encode the data\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', unknown_token=\"[UNK]\")\n",
    "train_encodings = tokenizer(train_text.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_text.tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
    "                                               torch.tensor(train_encodings['attention_mask']),\n",
    "                                               torch.tensor(train_labels))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
    "                                              torch.tensor(test_encodings['attention_mask']),\n",
    "                                              torch.tensor(test_labels))\n",
    "\n",
    "# Define the model\n",
    "# Set seed for model\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model.seed = seed_value\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Train the model\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "model.to(device)\n",
    "train(model, train_loader, optimizer)\n",
    "\n",
    "# Test the model\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "test(model, test_loader)\n",
    "\n",
    "# Save fine-tuned model to file\n",
    "model.save_pretrained('fine_tuned_distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = DistilBertForSequenceClassification.from_pretrained('fine_tuned_distilbert')\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained('fine_tuned_distilbert')\n",
    "\n",
    "# Evaluate fine-tuned model on evaluation data\n",
    "predicted = []\n",
    "for text in tqdm(test_text):\n",
    "    encoded_inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    output = loaded_model(**encoded_inputs)\n",
    "    predicted_label = torch.argmax(output.logits, dim=1).item()\n",
    "    predicted.append(predicted_label)\n",
    "\n",
    "print(classification_report(predicted, test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
